{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PenalizedMSELoss(nn.Module):\n",
    "    def __init__(self, penalty_factor=10.0):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.penalty_factor = penalty_factor\n",
    "\n",
    "    def forward(self, predictions, targets, mean=0, std=0):\n",
    "        mse_loss = self.mse(predictions, targets)\n",
    "        \n",
    "        # Penalty for negative predictions\n",
    "        penalty = torch.relu(-(predictions * std + mean)).sum()\n",
    "        total_loss = mse_loss + self.penalty_factor * penalty\n",
    "\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNC2PS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(3, 600)\n",
    "        self.fc2 = nn.Linear(600, 200)\n",
    "        self.fc3 = nn.Linear(200, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x)) \n",
    "        x = torch.relu(self.fc2(x)) \n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = NNC2PS().to(device)\n",
    "model = nn.DataParallel(model)\n",
    "print(f\"Using GPUs: {model.device_ids}\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "penalty_factor = 1.50e2\n",
    "criterion = PenalizedMSELoss(penalty_factor=penalty_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNC2PL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(3, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.fc5 = nn.Linear(128, 64)\n",
    "        self.fc6 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x)) \n",
    "        x = torch.relu(self.fc2(x)) \n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = torch.relu(self.fc5(x))\n",
    "        x = self.fc6(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "n_samples = 500000\n",
    "gamma_th = 5/3\n",
    "K = [8.9493e-02, None, None, None]  \n",
    "Gamma = [1.3569e+00, 3.0050e+00, 2.9880e+00, 2.8510e+00]\n",
    "rho_breaks = [2.3674e-04, 8.1147e-04, 1.6191e-03]\n",
    "\n",
    "rho_min = 2e-5\n",
    "rho_max = 2e-3\n",
    "rho = rho_min + (rho_max - rho_min) * torch.rand(n_samples, 1)\n",
    "vx  = 0.721 * torch.rand(n_samples, 1)\n",
    "W = (1 - vx**2).pow(-0.5)\n",
    "\n",
    "a_i = torch.zeros(len(K))\n",
    "for i in range(1, len(K)):\n",
    "    K[i] = K[i-1] * rho_breaks[i-1]**(Gamma[i-1] - Gamma[i])\n",
    "    a_i[i] = a_i[i-1] + K[i-1] * rho_breaks[i-1]**(Gamma[i-1] - 1) / (Gamma[i-1] - 1) - K[i] * rho_breaks[i-1]**(Gamma[i] - 1) / (Gamma[i] - 1)\n",
    "\n",
    "p = torch.zeros_like(rho)\n",
    "eps_cold = torch.zeros_like(rho)\n",
    "h = torch.zeros_like(rho)\n",
    "eps = torch.zeros_like(rho)\n",
    "for i in range(len(K)):\n",
    "    if i == 0:\n",
    "        mask = rho < rho_breaks[i]\n",
    "    elif i < len(K) - 1:\n",
    "        mask = (rho >= rho_breaks[i-1]) & (rho < rho_breaks[i])\n",
    "    else:\n",
    "        mask = rho >= rho_breaks[i-1]\n",
    "    \n",
    "    eps_cold[mask] = a_i[i] + K[i] * rho[mask]**(Gamma[i] - 1) / (Gamma[i] - 1)\n",
    "    p[mask] = K[i] * rho[mask]**Gamma[i]\n",
    "\n",
    "eps_min = 0\n",
    "eps_max = 2\n",
    "eps_th = eps_min + (eps_max - eps_min) * torch.rand(n_samples, 1)\n",
    "p_th = (gamma_th - 1) * rho * eps_th\n",
    "p += p_th\n",
    "h = 1 + eps_cold + eps_th + p / rho\n",
    "\n",
    "D = rho * W\n",
    "Sx = rho * h * W**2 * vx\n",
    "tau = rho * h * W**2 - p - D\n",
    "\n",
    "inputs = torch.cat((D, Sx, tau), dim=1)\n",
    "outputs = copy.deepcopy(p)\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(inputs, outputs)\n",
    "\n",
    "test_size = int(0.05 * n_samples)  # 5% for testing\n",
    "val_size = int(0.05 * n_samples)   # 5% for validation\n",
    "train_size = n_samples - test_size - val_size  # Remaining 90% for training\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "inputs_train_unscaled, outputs_train_unscaled = train_dataset[:]\n",
    "inputs_val_unscaled, outputs_val_unscaled = val_dataset[:]\n",
    "inputs_test_unscaled, outputs_test_unscaled = test_dataset[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_negative_outputs = torch.sum(torch.lt(outputs_train_unscaled, 0)).item()\n",
    "print(f'Number of Negative Outputs: {num_negative_outputs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_rho = np.log10(rho)\n",
    "log_p = np.log10(p)\n",
    "log_eps = np.log10(eps_cold + eps_th)\n",
    "log_h = np.log10(h)\n",
    "\n",
    "# Sort indices\n",
    "sort_indices = np.argsort(log_rho, axis=0).flatten()\n",
    "\n",
    "log_rho_sorted = log_rho[sort_indices]\n",
    "log_p_sorted = log_p[sort_indices]\n",
    "log_eps_sorted = log_eps[sort_indices]\n",
    "log_h_sorted = log_h[sort_indices]\n",
    "\n",
    "# fs = 22\n",
    "# fig, axs = plt.subplots(1, 3, figsize=(30, 10))\n",
    "\n",
    "# axs[0].scatter(log_rho_sorted, log_p_sorted, s=1)\n",
    "# for rho_break in rho_breaks:\n",
    "#     axs[0].axvline(x=np.log10(rho_break), color='r', linestyle='--')\n",
    "# axs[0].set_xlabel(r'$\\log \\rho$', fontsize=fs)\n",
    "# axs[0].set_ylabel(r'$\\log p$', fontsize=fs)\n",
    "# axs[0].set_title(r'$\\log \\rho$ vs $\\log p$', fontsize=fs)\n",
    "# axs[0].grid(True)\n",
    "\n",
    "# axs[1].scatter(log_rho_sorted, log_eps_sorted, s=1)\n",
    "# for rho_break in rho_breaks:\n",
    "#     axs[1].axvline(x=np.log10(rho_break), color='r', linestyle='--')\n",
    "# axs[1].set_xlabel(r'$\\log \\rho$', fontsize=fs)\n",
    "# axs[1].set_ylabel(r'$\\log \\epsilon$', fontsize=fs)\n",
    "# axs[1].set_title(r'$\\log \\rho$ vs $\\log \\epsilon$', fontsize=fs)\n",
    "# axs[1].grid(True)\n",
    "\n",
    "# axs[2].scatter(log_rho_sorted, log_h_sorted, s=1)\n",
    "# for rho_break in rho_breaks:\n",
    "#     axs[2].axvline(x=np.log10(rho_break), color='r', linestyle='--')\n",
    "# axs[2].set_xlabel(r'$\\log \\rho$', fontsize=fs)\n",
    "# axs[2].set_ylabel(r'$\\log h$', fontsize=fs)\n",
    "# axs[2].set_title(r'$\\log \\rho$ vs $\\log h$', fontsize=fs)\n",
    "# axs[2].grid(True)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('../images/piecewise_data_mpl.png', dpi=600)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def create_publication_plots(log_rho_sorted, log_p_sorted, log_eps_sorted, log_h_sorted, rho_breaks):\n",
    "    \"\"\"\n",
    "    Create publication-quality plots using seaborn for EOS-related data.\n",
    "    \"\"\"\n",
    "    # Set the style for publication-quality plots\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.set_context(\"paper\", font_scale=2)\n",
    "    \n",
    "    # Create figure with three subplots\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    \n",
    "    # Common plotting parameters\n",
    "    plot_params = dict(\n",
    "        marker='o',\n",
    "        s=10,  # Larger point size for better visibility\n",
    "        alpha=0.6,  # Some transparency\n",
    "        color=sns.color_palette(\"deep\")[0],  # Use seaborn's color palette\n",
    "        linewidth=0\n",
    "    )\n",
    "    \n",
    "    # Common axis parameters\n",
    "    def setup_axis(ax, x_label, y_label, title):\n",
    "        ax.tick_params(labelsize=18)\n",
    "        ax.set_xlabel(x_label, fontsize=24, fontweight='bold')\n",
    "        ax.set_ylabel(y_label, fontsize=24, fontweight='bold')\n",
    "        # ax.set_title(title, fontsize=24, pad=20)\n",
    "        \n",
    "        # Add vertical lines for density breaks\n",
    "        for rho_break in rho_breaks:\n",
    "            ax.axvline(x=np.log10(rho_break), color='red', \n",
    "                      linestyle='--', linewidth=2, alpha=0.7)\n",
    "    \n",
    "    # First subplot: log œÅ vs log p\n",
    "    ax1.scatter(log_rho_sorted, log_p_sorted, **plot_params)\n",
    "    setup_axis(ax1, r'$\\log \\rho$', r'$\\log p$', r'$\\log \\rho$ vs $\\log p$')\n",
    "    \n",
    "    # Second subplot: log œÅ vs log Œµ\n",
    "    ax2.scatter(log_rho_sorted, log_eps_sorted, **plot_params)\n",
    "    setup_axis(ax2, r'$\\log \\rho$', r'$\\log \\epsilon$', r'$\\log \\rho$ vs $\\log \\epsilon$')\n",
    "    \n",
    "    # Third subplot: log œÅ vs log h\n",
    "    ax3.scatter(log_rho_sorted, log_h_sorted, **plot_params)\n",
    "    setup_axis(ax3, r'$\\log \\rho$', r'$\\log h$', r'$\\log \\rho$ vs $\\log h$')\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    plt.savefig('../images/piecewise_data.png', bbox_inches='tight', dpi=600)\n",
    "    \n",
    "    return fig, (ax1, ax2, ax3)\n",
    "\n",
    "# Example usage:\n",
    "fig, axes = create_publication_plots(log_rho_sorted, log_p_sorted, \n",
    "                                     log_eps_sorted, log_h_sorted, rho_breaks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_scaler = StandardScaler()\n",
    "inputs_train = torch.from_numpy(input_scaler.fit_transform(inputs_train_unscaled))\n",
    "inputs_val = torch.from_numpy(input_scaler.transform(inputs_val_unscaled))\n",
    "inputs_test = torch.from_numpy(input_scaler.transform(inputs_test_unscaled))\n",
    "\n",
    "output_scaler = StandardScaler()\n",
    "outputs_train = torch.from_numpy(output_scaler.fit_transform(outputs_train_unscaled))\n",
    "outputs_val = torch.from_numpy(output_scaler.fit_transform(outputs_val_unscaled))\n",
    "outputs_test = torch.from_numpy(output_scaler.transform(outputs_test_unscaled))\n",
    "\n",
    "train_dataset = TensorDataset(inputs_train, outputs_train)\n",
    "val_dataset = TensorDataset(inputs_val, outputs_val)\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "n_epochs = 85 # 85 seems to be the ideal epoch for this model\n",
    "\n",
    "train_mean = torch.mean(outputs_train_unscaled, dim=0).to(device)\n",
    "train_std = torch.std(outputs_train_unscaled, unbiased=False).to(device)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Function to get the current learning rate\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_inputs, batch_outputs in train_loader:\n",
    "        batch_inputs = batch_inputs.float()\n",
    "        batch_outputs = batch_outputs.float()\n",
    "        \n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_outputs = batch_outputs.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        batch_outputs_pred = model(batch_inputs)\n",
    "        \n",
    "        loss = criterion(batch_outputs_pred, batch_outputs, train_mean, train_std)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * batch_inputs.size(0)\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_inputs, batch_outputs in val_loader:\n",
    "            batch_inputs = batch_inputs.float()\n",
    "            batch_outputs = batch_outputs.float()\n",
    "            \n",
    "            batch_inputs = batch_inputs.to(device)\n",
    "            batch_outputs = batch_outputs.to(device)\n",
    "            \n",
    "            batch_outputs_pred = model(batch_inputs)\n",
    "            loss = criterion(batch_outputs_pred, batch_outputs, train_mean, train_std)\n",
    "            \n",
    "            val_loss += loss.item() * batch_inputs.size(0)\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    current_lr = get_lr(optimizer)\n",
    "    print(f'Epoch {epoch+1}/{n_epochs} Train Loss: {train_loss:.16f} Val Loss: {val_loss:.16f} LR: {current_lr:.6f}')\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Check GPU memory allocation\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"Memory allocated on GPU {i}: {torch.cuda.memory_allocated(i)} bytes\")\n",
    "    \n",
    "    # Save checkpoint every 100 epochs\n",
    "    if (epoch + 1) % 200 == 0:\n",
    "        checkpoint_path = f'checkpoints/checkpoint_epoch_{epoch+1}.pth'\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': val_loss,\n",
    "        }, checkpoint_path)\n",
    "        print(f'Checkpoint saved at {checkpoint_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.semilogy(range(1, n_epochs + 1), train_losses, label='Training Loss')\n",
    "plt.semilogy(range(1, n_epochs + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss vs. Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.argmin(val_losses), np.min(val_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(model, torch.nn.DataParallel):\n",
    "    model = model.module\n",
    "    \n",
    "model = model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_standard_scaler(standardized_tensor, mean, std):\n",
    "    # Inverse standardize the tensor\n",
    "    original_tensor = (standardized_tensor * std) + mean\n",
    "    \n",
    "    return original_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure input and output tensors are on CPU\n",
    "inputs_test = inputs_test.to('cpu')\n",
    "outputs_test = outputs_test.to('cpu')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    predictions = model(inputs_test.float())\n",
    "    print(predictions[:10])\n",
    "inverted_predictions = inverse_standard_scaler(predictions, torch.mean(outputs_train_unscaled, dim=0), torch.std(outputs_train_unscaled, unbiased=False))\n",
    "\n",
    "# L1 Loss\n",
    "l1_loss = nn.L1Loss()\n",
    "l1_error = l1_loss(inverted_predictions, outputs_test_unscaled)\n",
    "print(f'L1 Error: {l1_error.item():.2e}')\n",
    "\n",
    "# Mean Squared Error (MSE) and L2 Loss\n",
    "mse_loss = nn.MSELoss()\n",
    "error = mse_loss(inverted_predictions, outputs_test_unscaled)\n",
    "l2_error = torch.sqrt(error)\n",
    "print(f'L2 Error: {l2_error.item():.2e}')\n",
    "\n",
    "# L-infinity norm (max absolute error)\n",
    "linf_error = torch.max(torch.abs(inverted_predictions - outputs_test_unscaled))\n",
    "print(f'L-infinity Error: {linf_error.item():.2e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inverted_predictions  # Check the length of the test outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_test_unscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert the predictions and make sure everything is physically meaningful\n",
    "# inverted_predictions = inverse_standard_scaler(predictions, torch.mean(outputs_test, dim=0), torch.std(outputs_test, unbiased=False))\n",
    "print(torch.sum(torch.lt(outputs_test, 0)).item())\n",
    "print(torch.sum(torch.lt(inverted_predictions, 0)).item())\n",
    "print(len([ip for ip in inverted_predictions if ip < 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean and standard deviation\n",
    "train_mean = train_mean.to(\"cpu\")\n",
    "train_std = train_std.to(\"cpu\")\n",
    "\n",
    "train_mean_out_np = train_mean.numpy()\n",
    "train_std_out_np = train_std.numpy()\n",
    "\n",
    "np.savetxt(\"./speed_test/gpu/mean_std_out_s_pen100.txt\", np.vstack((train_mean_out_np, train_std_out_np)), fmt=\"%.17f\")\n",
    "\n",
    "inputs_train_unscaled = inputs_train_unscaled.to(\"cpu\")\n",
    "\n",
    "mean = inputs_train_unscaled.mean(dim=0, keepdim=True)\n",
    "std = inputs_train_unscaled.std(dim=0, keepdim=True)\n",
    "\n",
    "train_mean_in_np = mean.numpy()\n",
    "train_std_in_np = std.numpy()\n",
    "np.savetxt(\"./speed_test/gpu/mean_std_in_s_pen100.txt\", np.vstack((train_mean_in_np, train_std_in_np)), fmt=\"%.17f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = os.path.join(\"..\", \"models\", \"piecewise_polytrope_nnc2pxl_cpu.pth\") # for CPU\n",
    "model_path = os.getcwd() + \"/piecewise_polytrope_nnc2ps_pen100_normalized_gpu.pth\" # for GPU\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "scripted_model = torch.jit.script(model)\n",
    "torch.jit.save(scripted_model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_test_unscaled_np = inputs_test_unscaled.cpu().numpy()\n",
    "outputs_test_unscaled_np = outputs_test_unscaled.cpu().numpy()\n",
    "inputs_test_np = inputs_test.cpu().numpy()\n",
    "outputs_test_np = outputs_test.cpu().numpy()\n",
    "preds_test_np = predictions.cpu().numpy()\n",
    "inverted_preds_np = inverted_predictions.cpu().numpy()\n",
    "inputs_train_unscaled_np = inputs_train_unscaled.cpu().numpy()\n",
    "outputs_train_unscaled_np = outputs_train_unscaled.cpu().numpy()\n",
    "\n",
    "# Save to txt with maximum precision\n",
    "np.savetxt('./speed_test/gpu/inputs_train_unscaled_s_pen100.txt', inputs_train_unscaled_np, fmt='%.9g')\n",
    "np.savetxt('./speed_test/gpu/outputs_train_unscaled_s_pen100.txt', outputs_train_unscaled_np, fmt='%.9g')\n",
    "np.savetxt('./speed_test/gpu/inputs_test_unscaled_s_pen100.txt', inputs_test_unscaled_np, fmt='%.9g')\n",
    "np.savetxt('./speed_test/gpu/outputs_test_unscaled_s_pen100.txt', outputs_test_unscaled_np, fmt='%.9g')\n",
    "np.savetxt('./speed_test/gpu/preds_test_s_pen100.txt', preds_test_np, fmt='%.9g')\n",
    "np.savetxt('./speed_test/gpu/inputs_test_scaled_s_pen100.txt', inputs_test_np, fmt='%.9g')\n",
    "np.savetxt('./speed_test/gpu/outputs_test_scaled_s_pen100.txt', outputs_test_np, fmt='%.9g')\n",
    "np.savetxt('./speed_test/gpu/inverted_preds_s_pen100.txt', inverted_preds_np, fmt='%.9g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
