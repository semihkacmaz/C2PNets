{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import h5py\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "# from sklearn import model_selection\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, random_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # cuda is still faster\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\") # Apple Silicon acceleration for torch 2.1.2 or later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eos_tables_dir = os.path.join(\"/NNC2P Workspace/eos_tables\")\n",
    "# eos_table_filename = \"LS180_234r_136t_50y_analmu_20091212_SVNr26.h5\"\n",
    "eos_table_filename = \"LS220_234r_136t_50y_analmu_20091212_SVNr26.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for obj in gc.get_objects():\n",
    "    if isinstance(obj, h5py.File):\n",
    "        try:\n",
    "            obj.close()\n",
    "        except:\n",
    "            continue\n",
    "        del obj\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_eos_table(filename):\n",
    "    return h5py.File(filename, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = os.path.join(eos_tables_dir, \"train_eos_table.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell is inspired and partially adapted by the following source: https://github.com/ThibeauWouters/master-thesis-AI\n",
    "\n",
    "V_MIN = 0\n",
    "V_MAX = 0.721\n",
    "\n",
    "def W(rho, eps, v, p = None):\n",
    "    v_sqr = v ** 2 if isinstance(v, float) else np.sum(v ** 2)\n",
    "    return (1 - v_sqr) ** (-1 / 2)\n",
    "\n",
    "def generate_training_data_c2p(eos_table, number_of_points, save_name):\n",
    "    \"\"\"\n",
    "    This function generates training data by sampling from a given equation of state (EOS) table and performing a \n",
    "    primitive to conserved (P2C) transformation. The generated data is saved in a .csv file. The function takes as \n",
    "    input an EOS table, the number of data points to be generated, and the name of the file where the data will be saved.\n",
    "    \"\"\"\n",
    "    ye_table, temp_table, rho_table, eps_table, p_table = eos_table[\"ye\"][()], eos_table[\"logtemp\"][()], eos_table[\"logrho\"][()], eos_table[\"logenergy\"][()], eos_table[\"logpress\"][()]\n",
    "    len_ye, len_temp, len_rho = eos_table[\"pointsye\"][()][0], eos_table[\"pointstemp\"][()][0], eos_table[\"pointsrho\"][()][0]\n",
    "\n",
    "    features, labels = [], []\n",
    "\n",
    "    for _ in range(number_of_points):\n",
    "        v = random.uniform(V_MIN, V_MAX)\n",
    "        ye_index, temp_index, rho_index = np.random.choice(len_ye), np.random.choice(len_temp), np.random.choice(len_rho)\n",
    "\n",
    "        ye, logtemp, logrho, logeps, logp = ye_table[ye_index], temp_table[temp_index], rho_table[rho_index], eps_table[ye_index, temp_index, rho_index], p_table[ye_index, temp_index, rho_index]\n",
    "        temp, rho, eps, p = 10 ** logtemp, 10 ** logrho, 10 ** logeps, 10 ** logp\n",
    "\n",
    "        h, D = 1 + eps + p / rho, rho * W(rho, eps, v, p)\n",
    "        S, tau =  rho * h * W(rho, eps, v, p) ** 2 * v, rho * h * W(rho, eps, v, p) ** 2 - p - D\n",
    "\n",
    "        features.append([np.log10(D), np.log10(S), np.log10(tau), ye])\n",
    "        labels.append([logp])\n",
    "\n",
    "    with h5py.File(save_name, 'w') as f:\n",
    "        f.create_dataset(\"features\", data=features)\n",
    "        f.create_dataset(\"labels\", data=labels)\n",
    "\n",
    "    eos_table.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "number_of_points = 1000000 # For speed 100000\n",
    "eos_table = read_eos_table(os.path.join(eos_tables_dir, eos_table_filename))\n",
    "generate_training_data_c2p(eos_table, number_of_points=number_of_points, save_name=os.path.join(eos_tables_dir, \"c2p_full_dataset.h5\"))\n",
    "eos_table.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # On DTAI only\n",
    "# def standard_scaler(input_tensor):\n",
    "#     mean = torch.mean(input_tensor, dim=0)\n",
    "#     std = torch.std(input_tensor, dim=0, unbiased=False)  # Match scikit-learn's behavior\n",
    "\n",
    "#     standardized_tensor = (input_tensor - mean) / std\n",
    "    \n",
    "#     return standardized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_c2p_table = h5py.File(os.path.join(eos_tables_dir, \"c2p_full_dataset.h5\"), 'r')\n",
    "features = train_c2p_table[\"features\"][:].astype(np.float32)\n",
    "labels = train_c2p_table[\"labels\"][:].astype(np.float32)\n",
    "train_c2p_table.close()\n",
    "\n",
    "features_tensor = torch.from_numpy(features)\n",
    "labels_tensor = torch.from_numpy(labels)\n",
    "\n",
    "val_size = number_of_points // 20\n",
    "test_size = number_of_points // 20\n",
    "train_size = number_of_points - val_size - test_size\n",
    "\n",
    "dataset = TensorDataset(features_tensor, labels_tensor)\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_features_unscaled, train_labels_unscaled = train_dataset[:]\n",
    "val_features_unscaled, val_labels_unscaled = val_dataset[:]\n",
    "test_features_unscaled, test_labels_unscaled = test_dataset[:]\n",
    "\n",
    "input_scaler = StandardScaler()\n",
    "train_features = torch.from_numpy(input_scaler.fit_transform(train_features_unscaled))\n",
    "val_features = torch.from_numpy(input_scaler.transform(val_features_unscaled))\n",
    "test_features = torch.from_numpy(input_scaler.transform(test_features_unscaled))\n",
    "\n",
    "output_scaler = StandardScaler()\n",
    "train_labels = torch.from_numpy(output_scaler.fit_transform(train_labels_unscaled))\n",
    "val_labels = torch.from_numpy(output_scaler.transform(val_labels_unscaled))\n",
    "test_labels = torch.from_numpy(output_scaler.transform(test_labels_unscaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_features, train_labels)\n",
    "val_dataset = TensorDataset(val_features, val_labels)\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NNC2P_Tabulated(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(4, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.fc5 = nn.Linear(128, 64)\n",
    "        self.fc6 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x)) \n",
    "        x = torch.relu(self.fc2(x)) \n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = torch.relu(self.fc5(x))\n",
    "        x = self.fc6(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = NNC2P_Tabulated().to(device)\n",
    "model = nn.DataParallel(model)\n",
    "print(f\"Using GPUs: {model.device_ids}\")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "n_epochs = 250\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Function to get the current learning rate\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "# early_stop_patience = 15  # Number of epochs to wait for improvement\n",
    "# min_delta_percent = 0.01  # Minimum % improvement (e.g., 1%)\n",
    "# best_val_loss = float('inf')\n",
    "# early_stop_counter = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_inputs, batch_outputs in train_loader:\n",
    "        batch_inputs = batch_inputs.float()\n",
    "        batch_outputs = batch_outputs.float()\n",
    "        \n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_outputs = batch_outputs.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        batch_outputs_pred = model(batch_inputs)\n",
    "        \n",
    "        loss = criterion(batch_outputs_pred, batch_outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * batch_inputs.size(0)\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_inputs, batch_outputs in val_loader:\n",
    "            batch_inputs = batch_inputs.float()\n",
    "            batch_outputs = batch_outputs.float()\n",
    "            \n",
    "            batch_inputs = batch_inputs.to(device)\n",
    "            batch_outputs = batch_outputs.to(device)\n",
    "            \n",
    "            batch_outputs_pred = model(batch_inputs)\n",
    "            loss = criterion(batch_outputs_pred, batch_outputs)\n",
    "            \n",
    "            val_loss += loss.item() * batch_inputs.size(0)\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # # Early stopping logic\n",
    "    # if best_val_loss == float('inf') or (best_val_loss - val_loss) / best_val_loss > min_delta_percent:\n",
    "    #     best_val_loss = val_loss\n",
    "    #     early_stop_counter = 0\n",
    "    # else:\n",
    "    #     early_stop_counter += 1\n",
    "    #     if early_stop_counter >= early_stop_patience:\n",
    "    #         print(f\"Early stopping at epoch {epoch+1} due to insufficient validation loss improvement.\")\n",
    "    #         break\n",
    "    \n",
    "    current_lr = get_lr(optimizer)\n",
    "    print(f'Epoch {epoch+1}/{n_epochs} Train Loss: {train_loss:.16f} Val Loss: {val_loss:.16f} LR: {current_lr:.6f}')\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Check GPU memory allocation\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"Memory allocated on GPU {i}: {torch.cuda.memory_allocated(i)} bytes\")\n",
    "    \n",
    "    # Save checkpoint every 125 epochs\n",
    "    if (epoch + 1) % 125 == 0:\n",
    "        checkpoint_path = f'checkpoints/checkpoint_tabulated_epoch_{epoch+1}.pth'\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': val_loss,\n",
    "        }, checkpoint_path)\n",
    "        print(f'Checkpoint saved at {checkpoint_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot training and validation loss 1M 1M\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.semilogy(range(1, n_epochs + 1), train_losses, label='Training Loss')\n",
    "plt.semilogy(range(1, n_epochs + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss vs. Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(np.argmin(val_losses), np.min(val_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if isinstance(model, torch.nn.DataParallel):\n",
    "    model = model.module\n",
    "    \n",
    "model = model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    predictions = model(test_features.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inverse_standard_scaler(standardized_tensor, mean, std):\n",
    "    # Inverse standardize the tensor\n",
    "    original_tensor = (standardized_tensor * std) + mean\n",
    "    \n",
    "    return original_tensor\n",
    "\n",
    "inverted_predictions = inverse_standard_scaler(predictions, torch.mean(train_labels_unscaled, dim=0), torch.std(train_labels_unscaled, unbiased=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "l1_loss = nn.L1Loss()\n",
    "l1_error = l1_loss(inverted_predictions, test_labels_unscaled)\n",
    "print(f'L1 Error: {l1_error.item():.2e}')\n",
    "linf_error = torch.max(torch.abs(inverted_predictions - test_labels_unscaled))\n",
    "print(f'L-infinity Error: {linf_error.item():.2e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(inverted_predictions, test_labels_unscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = os.path.join(\"NNC2P Workspace/models\", \"NNC2P_Tabulated.pth\")\n",
    "model.eval()\n",
    "model.to(device)\n",
    "scripted_model = torch.jit.script(model)\n",
    "torch.jit.save(scripted_model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_mean = torch.mean(train_labels_unscaled, dim=0).to(\"cpu\")\n",
    "train_std = torch.std(train_labels_unscaled, unbiased=False).to(\"cpu\")\n",
    "\n",
    "train_mean_out_np = train_mean.numpy()\n",
    "train_std_out_np = train_std.numpy()\n",
    "\n",
    "np.savetxt(\"./speed_test/gpu/mean_std_out_tabulated.txt\", np.vstack((train_mean_out_np, train_std_out_np)), fmt=\"%.17f\")\n",
    "\n",
    "train_features_unscaled = train_features_unscaled.to(\"cpu\")\n",
    "\n",
    "mean = train_features_unscaled.mean(dim=0, keepdim=True)\n",
    "std = train_features_unscaled.std(dim=0, keepdim=True)\n",
    "\n",
    "train_mean_in_np = mean.numpy()\n",
    "train_std_in_np = std.numpy()\n",
    "np.savetxt(\"./speed_test/gpu/mean_std_in_tabulated.txt\", np.vstack((train_mean_in_np, train_std_in_np)), fmt=\"%.17f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs_test_unscaled_np = test_features_unscaled.cpu().numpy()\n",
    "outputs_test_unscaled_np = test_labels_unscaled.cpu().numpy()\n",
    "inputs_test_np = test_features.cpu().numpy()\n",
    "outputs_test_np = test_labels.cpu().numpy()\n",
    "preds_test_np = predictions.cpu().numpy()\n",
    "inverted_preds_np = inverted_predictions.cpu().numpy()\n",
    "inputs_train_unscaled_np = train_features_unscaled.cpu().numpy()\n",
    "outputs_train_unscaled_np = train_labels_unscaled.cpu().numpy()\n",
    "\n",
    "# Save to txt with maximum precision\n",
    "np.savetxt('./speed_test/gpu/inputs_train_unscaled_tabulated.txt', inputs_train_unscaled_np, fmt='%.9g')\n",
    "np.savetxt('./speed_test/gpu/outputs_train_unscaled_tabulated.txt', outputs_train_unscaled_np, fmt='%.9g')\n",
    "np.savetxt('./speed_test/gpu/inputs_test_unscaled_tabulated.txt', inputs_test_unscaled_np, fmt='%.9g')\n",
    "np.savetxt('./speed_test/gpu/outputs_test_unscaled_tabulated.txt', outputs_test_unscaled_np, fmt='%.9g')\n",
    "np.savetxt('./speed_test/gpu/preds_test_tabulated.txt', preds_test_np, fmt='%.9g')\n",
    "np.savetxt('./speed_test/gpu/inputs_test_scaled_tabulated.txt', inputs_test_np, fmt='%.9g')\n",
    "np.savetxt('./speed_test/gpu/outputs_test_scaled_tabulated.txt', outputs_test_np, fmt='%.9g')\n",
    "np.savetxt('./speed_test/gpu/inverted_preds_tabulated.txt', inverted_preds_np, fmt='%.9g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_features_unscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LOAD MODEL\n",
    "model_path = \"NNC2P Workspace/models/NNC2P_Tabulated.pth\"\n",
    "model = torch.jit.load(model_path, map_location=device)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "loaded_stats_in = np.loadtxt(\"./speed_test/gpu/mean_std_in_tabulated.txt\")\n",
    "\n",
    "train_mean_in = loaded_stats_in[0] # read train input/features mean (1x4) \n",
    "train_std_in = loaded_stats_in[1]  # read train input/features std (1x4)\n",
    "\n",
    "loaded_stats_out = np.loadtxt(\"./speed_test/gpu/mean_std_out_tabulated.txt\")\n",
    "\n",
    "train_mean_out = loaded_stats_out[0] # read train output/label mean \n",
    "train_std_out = loaded_stats_out[1] # read train output/label std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_norm(predictions, y, reduction = True):\n",
    "    \"\"\"\n",
    "    Computes the L1 norm between predictions made by the neural network and the actual values.\n",
    "    :param predictions: Predictions made by the neural network architecture.\n",
    "    :param y: Actual values\n",
    "    :return: L1 norm between predictions and y\n",
    "    \"\"\"\n",
    "    if len(predictions) == 0:\n",
    "        print(\"Predictions is empty list\")\n",
    "        return 0\n",
    "    if len(predictions) != len(y):\n",
    "        print(\"Predictions and y must have same size\")\n",
    "        return 0\n",
    "\n",
    "    if reduction:\n",
    "        return np.sum(abs(predictions - y), axis=0)/len(predictions)\n",
    "    else:\n",
    "        return abs(predictions - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# JUST LOAD THE MODEL BEFORE THIS\n",
    "from matplotlib.colors import LogNorm\n",
    "fs = 16\n",
    "k_B = 8.617333262145e-11  # Boltzmann constant in MeV/K\n",
    "\n",
    "eos_table = read_eos_table(os.path.join(eos_tables_dir, eos_table_filename))\n",
    "ye = eos_table[\"ye\"][()]\n",
    "# ye_index = len(ye) // 2\n",
    "ye_index = 6\n",
    "ye_value = ye[ye_index]\n",
    "\n",
    "logrho       = eos_table[\"logrho\"][()]\n",
    "logtemp      = eos_table[\"logtemp\"][()]\n",
    "logpress     = eos_table[\"logpress\"][()]\n",
    "logenergy    = eos_table[\"logenergy\"][()]\n",
    "\n",
    "targets = logenergy[ye_index]\n",
    "targets = np.swapaxes(targets, 0, 1)\n",
    "n_logrho, n_logtemp = targets.shape\n",
    "\n",
    "logeps, logp = logenergy[ye_index], logpress[ye_index]\n",
    "\n",
    "W = 1.2\n",
    "v = np.sqrt((W ** 2 - 1)/W**2)\n",
    "\n",
    "eps, p = 10 ** logeps, 10 ** logp\n",
    "# print(eps.shape, p.shape)\n",
    "\n",
    "input_values = []\n",
    "for i, logtemp_value in enumerate(logtemp):\n",
    "    for j, logrho_value in enumerate(logrho):\n",
    "        temp, rho = 10 ** logtemp_value, 10 ** logrho_value\n",
    "        h, D = 1 + eps[i, j] + p[i, j] / rho, rho * W\n",
    "        S, tau = rho * h * W ** 2 * v, rho * h * W ** 2 - p[i, j] - D\n",
    "        new_row = [np.log10(D), np.log10(S), np.log10(tau), ye_value]\n",
    "        input_values.append(new_row)\n",
    "input_values = np.array(input_values)\n",
    "\n",
    "input_values = (input_values - train_mean_in) / train_std_in\n",
    "with torch.no_grad():\n",
    "    # input_values = input_scaler.transform(input_values)\n",
    "    input_values = torch.from_numpy(input_values).float()\n",
    "    predictions = model(input_values.to(device))\n",
    "    predictions = predictions.cpu().numpy()\n",
    "\n",
    "# predictions = inverse_standard_scaler(predictions, train_mean.cpu(), train_std.cpu())\n",
    "predictions = predictions * train_std_out + train_mean_out\n",
    "# predictions = predictions.numpy()\n",
    "norm_function = l1_norm\n",
    "\n",
    "sliced_predictions = predictions[:, 0]\n",
    "\n",
    "targets = logpress[ye_index]\n",
    "targets = np.swapaxes(targets, 0, 1)\n",
    "\n",
    "target_values = []\n",
    "for a in range(n_logtemp):\n",
    "    for b in range(n_logrho):\n",
    "        target_values.append(targets[b, a])\n",
    "\n",
    "target_values = np.array(target_values)\n",
    "\n",
    "delta_vals = norm_function(target_values, sliced_predictions, reduction=False)\n",
    "delta_vals = delta_vals.reshape((n_logtemp, n_logrho))\n",
    "delta_vals = delta_vals/target_values.reshape((n_logtemp, n_logrho))\n",
    "\n",
    "delta_vals = delta_vals/target_values.reshape((n_logtemp, n_logrho))\n",
    "\n",
    "fig = plt.figure(figsize=(4, 3))\n",
    "\n",
    "im = plt.imshow(delta_vals, origin=\"lower\", norm=LogNorm(vmin=np.min(delta_vals), vmax=np.max(delta_vals)))\n",
    "\n",
    "# Labels with a smaller font size\n",
    "plt.xlabel(r\"$\\log \\rho$\", fontsize=fs - 4)\n",
    "plt.ylabel(r\"$\\log T$\", fontsize=fs - 4)\n",
    "\n",
    "xt = [0, n_logrho // 2, n_logrho]\n",
    "xl = np.round([logrho[0], logrho[n_logrho // 2], logrho[-1]], 2)\n",
    "yt = [0, n_logtemp // 2, n_logtemp]\n",
    "yl_K = np.round(np.log10(10**np.array([logtemp[0], logtemp[n_logtemp // 2], logtemp[-1]]) / k_B), 2)\n",
    "\n",
    "# Adjusting tick labels with smaller font size\n",
    "plt.xticks(xt, xl, fontsize=fs - 6)\n",
    "plt.yticks(yt, yl_K, fontsize=fs - 6)\n",
    "\n",
    "# Color bar with a reduced label font size\n",
    "cbar = plt.colorbar(im, shrink=0.7)\n",
    "cbar.set_label('Relative Error', rotation=270, labelpad=20, fontsize=fs - 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_table = read_eos_table(os.path.join(eos_tables_dir, eos_table_filename))\n",
    "ye = eos_table[\"ye\"][()]\n",
    "# ye_index = len(ye) // 2\n",
    "ye_index = 6\n",
    "ye_value = ye[ye_index]\n",
    "\n",
    "logrho       = eos_table[\"logrho\"][()]\n",
    "logtemp      = eos_table[\"logtemp\"][()]\n",
    "logpress     = eos_table[\"logpress\"][()]\n",
    "logenergy    = eos_table[\"logenergy\"][()]\n",
    "\n",
    "targets = logenergy[ye_index]\n",
    "targets = np.swapaxes(targets, 0, 1)\n",
    "n_logrho, n_logtemp = targets.shape\n",
    "\n",
    "logeps, logp = logenergy[ye_index], logpress[ye_index]\n",
    "\n",
    "W = 1.2\n",
    "v = np.sqrt((W ** 2 - 1)/W**2)\n",
    "\n",
    "temp, rho, eps, p = 10 ** logtemp, 10 ** logrho, 10 ** logeps, 10 ** logp\n",
    "\n",
    "h, D = 1 + eps + p / rho, rho * W\n",
    "S, tau = rho * h * W ** 2 * v, rho * h * W ** 2 - p - D\n",
    "\n",
    "input_values = []\n",
    "for i, logtemp_value in enumerate(logtemp):\n",
    "    for j, logrho_value in enumerate(logrho):\n",
    "        new_row = [np.log10(D[j]), np.log10(S[i, j]), np.log10(tau[i, j]), ye_value]\n",
    "        input_values.append(new_row)\n",
    "input_values = np.array(input_values)\n",
    "\n",
    "input_values = (input_values - train_mean_in) / train_std_in\n",
    "with torch.no_grad():\n",
    "    # input_values = input_scaler.transform(input_values)\n",
    "    input_values = torch.from_numpy(input_values).float().to(device)\n",
    "    predictions = model(input_values)\n",
    "    predictions = predictions.cpu().numpy()\n",
    "\n",
    "# predictions = inverse_standard_scaler(predictions, train_mean.cpu(), train_std.cpu())\n",
    "# predictions = predictions.numpy()\n",
    "predictions = predictions * train_std_out + train_mean_out\n",
    "norm_function = l1_norm\n",
    "\n",
    "sliced_predictions = predictions[:, 0]\n",
    "\n",
    "targets = logpress[ye_index]\n",
    "targets = np.swapaxes(targets, 0, 1)\n",
    "\n",
    "target_values = []\n",
    "for a in range(n_logtemp):\n",
    "    for b in range(n_logrho):\n",
    "        target_values.append(targets[b, a])\n",
    "\n",
    "target_values = np.array(target_values)\n",
    "\n",
    "delta_vals = norm_function(target_values, sliced_predictions, reduction=False)\n",
    "delta_vals = delta_vals.reshape((n_logtemp, n_logrho))\n",
    "delta_vals = delta_vals/target_values.reshape((n_logtemp, n_logrho))\n",
    "\n",
    "delta_vals = delta_vals/target_values.reshape((n_logtemp, n_logrho))\n",
    "print(delta_vals)\n",
    "\n",
    "fig = plt.figure(figsize=(9,4))\n",
    "\n",
    "im = plt.imshow(delta_vals, origin=\"lower\", norm=LogNorm(vmin=np.min(delta_vals), vmax=np.max(delta_vals)))\n",
    "\n",
    "plt.xlabel(r\"$\\log \\rho$\", fontsize=fs)\n",
    "plt.ylabel(r\"$\\log T$\", fontsize=fs)\n",
    "\n",
    "xt = [0, n_logrho//2, n_logrho]\n",
    "xl = np.round([logrho[0], logrho[n_logrho // 2], logrho[-1]], 2)\n",
    "yt = [0, n_logtemp//2, n_logtemp]\n",
    "yl = np.round([logtemp[0], logtemp[n_logtemp // 2], logtemp[-1]], 2)\n",
    "\n",
    "plt.xticks(xt, xl)\n",
    "plt.yticks(yt, yl)\n",
    "\n",
    "cbar = plt.colorbar(im, shrink=0.7)\n",
    "cbar.set_label('Delta Values', rotation=270, labelpad=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_table = read_eos_table(os.path.join(eos_tables_dir, eos_table_filename))\n",
    "ye = eos_table[\"ye\"][()]\n",
    "ye_index = 6\n",
    "ye_value = ye[ye_index]\n",
    "\n",
    "k_B = 8.617333262145e-11  # Boltzmann constant in MeV/K\n",
    "\n",
    "logrho       = eos_table[\"logrho\"][()]\n",
    "logtemp      = eos_table[\"logtemp\"][()]\n",
    "logpress     = eos_table[\"logpress\"][()]\n",
    "logenergy    = eos_table[\"logenergy\"][()]\n",
    "\n",
    "targets = logenergy[ye_index]\n",
    "targets = np.swapaxes(targets, 0, 1)\n",
    "n_logrho, n_logtemp = targets.shape\n",
    "\n",
    "logeps, logp = logenergy[ye_index], logpress[ye_index]\n",
    "\n",
    "# W_values = [1.6, 1.8, 2.0, 2.2, 2.4]\n",
    "W_values = [1.1, 1.2, 1.3, 1.4]\n",
    "\n",
    "for W in W_values:\n",
    "    v = np.sqrt((W ** 2 - 1)/W**2)\n",
    "\n",
    "    temp, rho, eps, p = 10 ** logtemp, 10 ** logrho, 10 ** logeps, 10 ** logp\n",
    "\n",
    "    h, D = 1 + eps + p / rho, rho * W\n",
    "    S, tau = rho * h * W ** 2 * v, rho * h * W ** 2 - p - D\n",
    "\n",
    "    input_values = []\n",
    "    for i, logtemp_value in enumerate(logtemp):\n",
    "        for j, logrho_value in enumerate(logrho):\n",
    "            new_row = [np.log10(D[j]), np.log10(S[i, j]), np.log10(tau[i, j]), ye_value]\n",
    "            input_values.append(new_row)\n",
    "    input_values = np.array(input_values)\n",
    "    input_values = (input_values - train_mean_in) / train_std_in\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # input_values = scaler.transform(input_values)\n",
    "        input_values = torch.from_numpy(input_values).float().to(device)\n",
    "        predictions = model(input_values)\n",
    "        predictions = predictions.cpu().numpy()\n",
    "\n",
    "    predictions = predictions * train_std_out + train_mean_out\n",
    "    norm_function = l1_norm\n",
    "    sliced_predictions = predictions[:, 0]\n",
    "\n",
    "    targets = logpress[ye_index]\n",
    "    targets = np.swapaxes(targets, 0, 1)\n",
    "\n",
    "    target_values = []\n",
    "    for a in range(n_logtemp):\n",
    "        for b in range(n_logrho):\n",
    "            target_values.append(targets[b, a])\n",
    "\n",
    "    target_values = np.array(target_values)\n",
    "\n",
    "    delta_vals = norm_function(target_values, sliced_predictions, reduction=False)\n",
    "    delta_vals = delta_vals.reshape((n_logtemp, n_logrho))\n",
    "    delta_vals = delta_vals/target_values.reshape((n_logtemp, n_logrho))\n",
    "\n",
    "    delta_vals = delta_vals/target_values.reshape((n_logtemp, n_logrho))\n",
    "\n",
    "    fig = plt.figure(figsize=(9,4))\n",
    "\n",
    "    im = plt.imshow(delta_vals, origin=\"lower\", norm=LogNorm(vmin=np.min(delta_vals), vmax=np.max(delta_vals)))\n",
    "\n",
    "    plt.xlabel(r\"$\\log \\rho$\", fontsize=fs)\n",
    "    plt.ylabel(r\"$\\log T$\", fontsize=fs)\n",
    "\n",
    "    xt = [0, n_logrho//2, n_logrho]\n",
    "    xl = np.round([logrho[0], logrho[n_logrho // 2], logrho[-1]], 2)\n",
    "    yt = [0, n_logtemp//2, n_logtemp]\n",
    "    yl_K = np.round(np.log10( 10**np.array([logtemp[0], logtemp[n_logtemp // 2], logtemp[-1]]) / k_B), 2)\n",
    "\n",
    "    plt.xticks(xt, xl)\n",
    "    plt.yticks(yt, yl_K)\n",
    "\n",
    "    cbar = plt.colorbar(im, shrink=0.7)\n",
    "    cbar.set_label('Delta Values', rotation=270, labelpad=20)\n",
    "    plt.title(f'W = {W}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_table = read_eos_table(os.path.join(eos_tables_dir, eos_table_filename))\n",
    "ye = eos_table[\"ye\"][()]\n",
    "ye_index = 6\n",
    "ye_value = ye[ye_index]\n",
    "\n",
    "k_B = 8.617333262145e-11  # Boltzmann constant in MeV/K\n",
    "\n",
    "logrho       = eos_table[\"logrho\"][()]\n",
    "logtemp      = eos_table[\"logtemp\"][()]\n",
    "logpress     = eos_table[\"logpress\"][()]\n",
    "logenergy    = eos_table[\"logenergy\"][()]\n",
    "\n",
    "targets = logenergy[ye_index]\n",
    "targets = np.swapaxes(targets, 0, 1)\n",
    "n_logrho, n_logtemp = targets.shape\n",
    "\n",
    "logeps, logp = logenergy[ye_index], logpress[ye_index]\n",
    "\n",
    "# W_values = [1.6, 1.8, 2.0, 2.2, 2.4]\n",
    "W_values = [1.02, 1.1, 1.25, 1.4]\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(15, 4), sharey=True)\n",
    "\n",
    "for ax, W in zip(axs, W_values):\n",
    "    v = np.sqrt((W ** 2 - 1)/W**2)\n",
    "\n",
    "    temp, rho, eps, p = 10 ** logtemp, 10 ** logrho, 10 ** logeps, 10 ** logp\n",
    "\n",
    "    h, D = 1 + eps + p / rho, rho * W\n",
    "    S, tau = rho * h * W ** 2 * v, rho * h * W ** 2 - p - D\n",
    "\n",
    "    input_values = []\n",
    "    for i, logtemp_value in enumerate(logtemp):\n",
    "        for j, logrho_value in enumerate(logrho):\n",
    "            new_row = [np.log10(D[j]), np.log10(S[i, j]), np.log10(tau[i, j]), ye_value]\n",
    "            input_values.append(new_row)\n",
    "    input_values = np.array(input_values)\n",
    "    input_values = (input_values - train_mean_in) / train_std_in\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # input_values = scaler.transform(input_values)\n",
    "        input_values = torch.from_numpy(input_values).float().to(device)\n",
    "        predictions = model(input_values)\n",
    "        predictions = predictions.cpu().numpy()\n",
    "    \n",
    "    predictions = predictions * train_std_out + train_mean_out\n",
    "    norm_function = l1_norm\n",
    "\n",
    "    sliced_predictions = predictions[:, 0]\n",
    "\n",
    "    targets = logpress[ye_index]\n",
    "    targets = np.swapaxes(targets, 0, 1)\n",
    "\n",
    "    target_values = []\n",
    "    for a in range(n_logtemp):\n",
    "        for b in range(n_logrho):\n",
    "            target_values.append(targets[b, a])\n",
    "\n",
    "    target_values = np.array(target_values)\n",
    "\n",
    "    delta_vals = norm_function(target_values, sliced_predictions, reduction=False)\n",
    "    delta_vals = delta_vals.reshape((n_logtemp, n_logrho))\n",
    "    delta_vals = delta_vals/target_values.reshape((n_logtemp, n_logrho))\n",
    "\n",
    "    im = ax.imshow(delta_vals, origin=\"lower\", norm=LogNorm(vmin=np.min(delta_vals), vmax=np.max(delta_vals)))\n",
    "\n",
    "    ax.set_xlabel(r\"$\\log \\rho$\", fontsize=fs-6)\n",
    "    if W == W_values[0]:\n",
    "        ax.set_ylabel(r\"$\\log T$\", fontsize=fs-6)\n",
    "        \n",
    "    # if W == W_values[-1]:\n",
    "    #    ax.set_xlabel(r\"$\\log \\rho$\", fontsize=fs)\n",
    "\n",
    "    xt = [0, n_logrho//2, n_logrho]\n",
    "    xl = np.round([logrho[0], logrho[n_logrho // 2], logrho[-1]], 2)\n",
    "    yt = [0, n_logtemp//2, n_logtemp]\n",
    "    yl_K = np.round(np.log10( 10**np.array([logtemp[0], logtemp[n_logtemp // 2], logtemp[-1]]) / k_B), 2)\n",
    "\n",
    "    ax.set_xticks(xt)\n",
    "    ax.set_xticklabels(xl)\n",
    "    ax.set_yticks(yt)\n",
    "    ax.set_yticklabels(yl_K)\n",
    "\n",
    "    ax.set_title(f'W = {W}', fontsize=fs-6)\n",
    "\n",
    "fig.colorbar(im, ax=axs.ravel().tolist(), shrink=0.4, label='Relative Error')\n",
    "plt.savefig('../images/plot.pdf')  # Save the figure with a higher DPI for better resolution\n",
    "plt.savefig('../images/plot.png', dpi=300)\n",
    "plt.savefig('../images/plot.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import os\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "\n",
    "fs = 16\n",
    "\n",
    "def load_engine(engine_path):\n",
    "    \"\"\"Load TensorRT engine\"\"\"\n",
    "    logger = trt.Logger(trt.Logger.WARNING)\n",
    "    with open(engine_path, \"rb\") as f, trt.Runtime(logger) as runtime:\n",
    "        return runtime.deserialize_cuda_engine(f.read())\n",
    "\n",
    "def allocate_buffers(engine, batch_size=1):\n",
    "    \"\"\"Allocate device buffers and return input/output bindings\"\"\"\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    bindings = []\n",
    "    stream = cuda.Stream()\n",
    "    \n",
    "    for binding in engine:\n",
    "        shape = engine.get_binding_shape(binding)\n",
    "        size = trt.volume(shape) * batch_size\n",
    "        dtype = np.float32\n",
    "        # Allocate host and device buffers\n",
    "        host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "        device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "        # Append the device buffer to bindings\n",
    "        bindings.append(int(device_mem))\n",
    "        if engine.binding_is_input(binding):\n",
    "            inputs.append({'host': host_mem, 'device': device_mem, 'shape': shape})\n",
    "        else:\n",
    "            outputs.append({'host': host_mem, 'device': device_mem, 'shape': shape})\n",
    "    return inputs, outputs, bindings, stream\n",
    "\n",
    "def process_batch(context, input_data, inputs, outputs, bindings, stream):\n",
    "    \"\"\"Process a batch of data\"\"\"\n",
    "    # Prepare input data\n",
    "    input_buffer = np.zeros(inputs[0]['shape'], dtype=np.float32)\n",
    "    input_buffer[:input_data.shape[0]] = input_data\n",
    "    \n",
    "    np.copyto(inputs[0]['host'], input_buffer.ravel())\n",
    "    cuda.memcpy_htod_async(inputs[0]['device'], inputs[0]['host'], stream)\n",
    "    \n",
    "    # Run inference\n",
    "    context.execute_async_v2(bindings=bindings, stream_handle=stream.handle)\n",
    "    \n",
    "    # Transfer predictions back\n",
    "    cuda.memcpy_dtoh_async(outputs[0]['host'], outputs[0]['device'], stream)\n",
    "    stream.synchronize()\n",
    "    \n",
    "    # Only return the relevant portion of the output\n",
    "    return outputs[0]['host'][:input_data.shape[0]]\n",
    "\n",
    "# Load your EOS table and prepare data\n",
    "eos_table = read_eos_table(os.path.join(eos_tables_dir, eos_table_filename))\n",
    "ye = eos_table[\"ye\"][()]\n",
    "ye_index = 6\n",
    "ye_value = ye[ye_index]\n",
    "k_B = 8.617333262145e-11  # Boltzmann constant in MeV/K\n",
    "logrho = eos_table[\"logrho\"][()]\n",
    "logtemp = eos_table[\"logtemp\"][()]\n",
    "logpress = eos_table[\"logpress\"][()]\n",
    "logenergy = eos_table[\"logenergy\"][()]\n",
    "targets = logenergy[ye_index]\n",
    "targets = np.swapaxes(targets, 0, 1)\n",
    "n_logrho, n_logtemp = targets.shape\n",
    "logeps, logp = logenergy[ye_index], logpress[ye_index]\n",
    "\n",
    "# Load TensorRT engine\n",
    "engine_path = \"../models/NNC2P_Tabulated_FP16.engine\"  # Replace with your engine path\n",
    "engine = load_engine(engine_path)\n",
    "context = engine.create_execution_context()\n",
    "inputs, outputs, bindings, stream = allocate_buffers(engine)\n",
    "\n",
    "W_values = [1.02, 1.1, 1.25, 1.4]\n",
    "fig, axs = plt.subplots(1, 4, figsize=(15, 4), sharey=True)\n",
    "\n",
    "for ax, W in zip(axs, W_values):\n",
    "    v = np.sqrt((W ** 2 - 1)/W**2)\n",
    "    temp, rho, eps, p = 10 ** logtemp, 10 ** logrho, 10 ** logeps, 10 ** logp\n",
    "    h, D = 1 + eps + p / rho, rho * W\n",
    "    S, tau = rho * h * W ** 2 * v, rho * h * W ** 2 - p - D\n",
    "    \n",
    "    input_values = []\n",
    "    for i, logtemp_value in enumerate(logtemp):\n",
    "        for j, logrho_value in enumerate(logrho):\n",
    "            new_row = [np.log10(D[j]), np.log10(S[i, j]), np.log10(tau[i, j]), ye_value]\n",
    "            input_values.append(new_row)\n",
    "    \n",
    "    input_values = np.array(input_values, dtype=np.float32)\n",
    "    input_values = (input_values - train_mean_in) / train_std_in\n",
    "    \n",
    "    # Process the data\n",
    "    predictions = process_batch(context, input_values, inputs, outputs, bindings, stream)\n",
    "    predictions = predictions.reshape(-1, 1)\n",
    "    predictions = predictions * train_std_out + train_mean_out\n",
    "    \n",
    "    norm_function = l1_norm\n",
    "    sliced_predictions = predictions[:, 0]\n",
    "    targets = logpress[ye_index]\n",
    "    targets = np.swapaxes(targets, 0, 1)\n",
    "    target_values = []\n",
    "    for a in range(n_logtemp):\n",
    "        for b in range(n_logrho):\n",
    "            target_values.append(targets[b, a])\n",
    "    \n",
    "    target_values = np.array(target_values)\n",
    "    delta_vals = norm_function(target_values, sliced_predictions, reduction=False)\n",
    "    delta_vals = delta_vals.reshape((n_logtemp, n_logrho))\n",
    "    delta_vals = delta_vals/target_values.reshape((n_logtemp, n_logrho))\n",
    "    \n",
    "    im = ax.imshow(delta_vals, origin=\"lower\", norm=LogNorm(vmin=np.min(delta_vals), vmax=np.max(delta_vals)))\n",
    "    ax.set_xlabel(r\"$\\log \\rho$\", fontsize=fs-6)\n",
    "    if W == W_values[0]:\n",
    "        ax.set_ylabel(r\"$\\log T$\", fontsize=fs-6)\n",
    "    \n",
    "    xt = [0, n_logrho//2, n_logrho]\n",
    "    xl = np.round([logrho[0], logrho[n_logrho // 2], logrho[-1]], 2)\n",
    "    yt = [0, n_logtemp//2, n_logtemp]\n",
    "    yl_K = np.round(np.log10(10**np.array([logtemp[0], logtemp[n_logtemp // 2], logtemp[-1]]) / k_B), 2)\n",
    "    ax.set_xticks(xt)\n",
    "    ax.set_xticklabels(xl)\n",
    "    ax.set_yticks(yt)\n",
    "    ax.set_yticklabels(yl_K)\n",
    "    ax.set_title(f'W = {W}', fontsize=fs-6)\n",
    "\n",
    "# Clean up\n",
    "for input_buffer in inputs:\n",
    "    input_buffer['device'].free()\n",
    "for output_buffer in outputs:\n",
    "    output_buffer['device'].free()\n",
    "\n",
    "fig.colorbar(im, ax=axs.ravel().tolist(), shrink=0.4, label='Relative Error')\n",
    "# plt.savefig('../images/plot.pdf')\n",
    "# plt.savefig('../images/plot.png', dpi=300)\n",
    "# plt.savefig('../images/plot.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import os\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import torch\n",
    "\n",
    "fs = 22\n",
    "\n",
    "# [Previous helper functions remain the same: load_engine, allocate_buffers, process_batch]\n",
    "def load_engine(engine_path):\n",
    "    \"\"\"Load TensorRT engine\"\"\"\n",
    "    logger = trt.Logger(trt.Logger.WARNING)\n",
    "    with open(engine_path, \"rb\") as f, trt.Runtime(logger) as runtime:\n",
    "        return runtime.deserialize_cuda_engine(f.read())\n",
    "\n",
    "def allocate_buffers(engine, batch_size=1):\n",
    "    \"\"\"Allocate device buffers and return input/output bindings\"\"\"\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    bindings = []\n",
    "    stream = cuda.Stream()\n",
    "    \n",
    "    for binding in engine:\n",
    "        shape = engine.get_binding_shape(binding)\n",
    "        size = trt.volume(shape) * batch_size\n",
    "        dtype = np.float32\n",
    "        host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "        device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "        bindings.append(int(device_mem))\n",
    "        if engine.binding_is_input(binding):\n",
    "            inputs.append({'host': host_mem, 'device': device_mem, 'shape': shape})\n",
    "        else:\n",
    "            outputs.append({'host': host_mem, 'device': device_mem, 'shape': shape})\n",
    "    return inputs, outputs, bindings, stream\n",
    "\n",
    "def process_batch(context, input_data, inputs, outputs, bindings, stream):\n",
    "    \"\"\"Process a batch of data\"\"\"\n",
    "    input_buffer = np.zeros(inputs[0]['shape'], dtype=np.float32)\n",
    "    input_buffer[:input_data.shape[0]] = input_data\n",
    "    \n",
    "    np.copyto(inputs[0]['host'], input_buffer.ravel())\n",
    "    cuda.memcpy_htod_async(inputs[0]['device'], inputs[0]['host'], stream)\n",
    "    \n",
    "    context.execute_async_v2(bindings=bindings, stream_handle=stream.handle)\n",
    "    \n",
    "    cuda.memcpy_dtoh_async(outputs[0]['host'], outputs[0]['device'], stream)\n",
    "    stream.synchronize()\n",
    "    \n",
    "    return outputs[0]['host'][:input_data.shape[0]]\n",
    "\n",
    "# Load EOS table and prepare data\n",
    "eos_table = read_eos_table(os.path.join(eos_tables_dir, eos_table_filename))\n",
    "ye = eos_table[\"ye\"][()]\n",
    "ye_index = 6\n",
    "ye_value = ye[ye_index]\n",
    "k_B = 8.617333262145e-11  # Boltzmann constant in MeV/K\n",
    "logrho = eos_table[\"logrho\"][()]\n",
    "logtemp = eos_table[\"logtemp\"][()]\n",
    "logpress = eos_table[\"logpress\"][()]\n",
    "logenergy = eos_table[\"logenergy\"][()]\n",
    "targets = logenergy[ye_index]\n",
    "targets = np.swapaxes(targets, 0, 1)\n",
    "n_logrho, n_logtemp = targets.shape\n",
    "logeps, logp = logenergy[ye_index], logpress[ye_index]\n",
    "\n",
    "# Load TensorRT engines\n",
    "engine_fp32_path = \"../models/NNC2P_Tabulated.engine\"\n",
    "engine_fp16_path = \"../models/NNC2P_Tabulated_FP16.engine\"\n",
    "\n",
    "engine_fp32 = load_engine(engine_fp32_path)\n",
    "engine_fp16 = load_engine(engine_fp16_path)\n",
    "\n",
    "context_fp32 = engine_fp32.create_execution_context()\n",
    "context_fp16 = engine_fp16.create_execution_context()\n",
    "\n",
    "inputs_fp32, outputs_fp32, bindings_fp32, stream_fp32 = allocate_buffers(engine_fp32)\n",
    "inputs_fp16, outputs_fp16, bindings_fp16, stream_fp16 = allocate_buffers(engine_fp16)\n",
    "\n",
    "# Create figure with adjusted spacing\n",
    "W_values = [1.02, 1.1, 1.25, 1.4]\n",
    "fig = plt.figure(figsize=(16, 8))  # Reduced height\n",
    "plt.subplots_adjust(hspace=0.05, wspace=0.3)  # Much tighter vertical spacing\n",
    "gs = fig.add_gridspec(3, 4, left=0.1, right=0.9, top=0.95, bottom=0.08)\n",
    "axs = np.array([[fig.add_subplot(gs[i, j]) for j in range(4)] for i in range(3)])\n",
    "\n",
    "# Define models to process\n",
    "models = [\n",
    "    (\"PyTorch\", model, None, None, None, None),\n",
    "    (\"TensorRT FP32\", None, engine_fp32, context_fp32, inputs_fp32, bindings_fp32),\n",
    "    (\"TensorRT FP16\", None, engine_fp16, context_fp16, inputs_fp16, bindings_fp16)\n",
    "]\n",
    "\n",
    "all_delta_vals = []\n",
    "for row, (model_name, pytorch_model, engine, context, inputs, bindings) in enumerate(models):\n",
    "    for col, W in enumerate(W_values):\n",
    "        v = np.sqrt((W ** 2 - 1)/W**2)\n",
    "        temp, rho, eps, p = 10 ** logtemp, 10 ** logrho, 10 ** logeps, 10 ** logp\n",
    "        h, D = 1 + eps + p / rho, rho * W\n",
    "        S, tau = rho * h * W ** 2 * v, rho * h * W ** 2 - p - D\n",
    "        \n",
    "        input_values = []\n",
    "        for i, logtemp_value in enumerate(logtemp):\n",
    "            for j, logrho_value in enumerate(logrho):\n",
    "                new_row = [np.log10(D[j]), np.log10(S[i, j]), np.log10(tau[i, j]), ye_value]\n",
    "                input_values.append(new_row)\n",
    "        \n",
    "        input_values = np.array(input_values, dtype=np.float32)\n",
    "        input_values = (input_values - train_mean_in) / train_std_in\n",
    "        \n",
    "        # Get predictions based on model type\n",
    "        if model_name == \"PyTorch\":\n",
    "            with torch.no_grad():\n",
    "                input_values_torch = torch.from_numpy(input_values).float().to(device)\n",
    "                predictions = pytorch_model(input_values_torch)\n",
    "                predictions = predictions.cpu().numpy()\n",
    "        else:\n",
    "            # Use appropriate stream for each TensorRT model\n",
    "            stream = stream_fp32 if model_name == \"TensorRT FP32\" else stream_fp16\n",
    "            predictions = process_batch(context, input_values, inputs, outputs_fp32 if model_name == \"TensorRT FP32\" else outputs_fp16, \n",
    "                                     bindings, stream)\n",
    "            predictions = predictions.reshape(-1, 1)\n",
    "            \n",
    "        predictions = predictions * train_std_out + train_mean_out\n",
    "        \n",
    "        norm_function = l1_norm\n",
    "        sliced_predictions = predictions[:, 0]\n",
    "        targets = logpress[ye_index]\n",
    "        targets = np.swapaxes(targets, 0, 1)\n",
    "        target_values = []\n",
    "        for a in range(n_logtemp):\n",
    "            for b in range(n_logrho):\n",
    "                target_values.append(targets[b, a])\n",
    "        \n",
    "        target_values = np.array(target_values)\n",
    "        delta_vals = norm_function(target_values, sliced_predictions, reduction=False)\n",
    "        delta_vals = delta_vals.reshape((n_logtemp, n_logrho))\n",
    "        delta_vals = delta_vals/target_values.reshape((n_logtemp, n_logrho))\n",
    "        all_delta_vals.append(delta_vals)\n",
    "        \n",
    "        ax = axs[row, col]\n",
    "        im = ax.imshow(delta_vals, origin=\"lower\", norm=LogNorm(vmin=np.min(delta_vals), vmax=np.max(delta_vals)))\n",
    "        \n",
    "        # Set labels and ticks\n",
    "        if row == 2:  # Bottom row\n",
    "            ax.set_xlabel(r\"$\\log \\rho [g/cm^3]$\", fontsize=fs-6)\n",
    "        if col == 0:  # First column\n",
    "            ax.set_ylabel(r\"$\\log T [K]$\", fontsize=fs-6)\n",
    "            \n",
    "        xt = [0, n_logrho//2, n_logrho - 1]\n",
    "        xl = np.round([logrho[0], logrho[n_logrho // 2], logrho[-1]], 2)\n",
    "        yt = [0, n_logtemp//2, n_logtemp - 1]\n",
    "        yl_K = np.round(np.log10(10**np.array([logtemp[0], logtemp[n_logtemp // 2], logtemp[-1]]) / k_B), 2)\n",
    "        \n",
    "        # Only set xticks for bottom row\n",
    "        if row == 2:\n",
    "            ax.set_xticks(xt)\n",
    "            ax.set_xticklabels(xl)\n",
    "        else:\n",
    "            ax.set_xticks([])\n",
    "            \n",
    "        # Only set yticks for first column\n",
    "        if col == 0:\n",
    "            ax.set_yticks(yt)\n",
    "            ax.set_yticklabels(yl_K)\n",
    "        else:\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        # Set title only for top row\n",
    "        if row == 0:\n",
    "            ax.set_title(f'W = {W}', fontsize=fs-6)\n",
    "\n",
    "# Get global vmin and vmax for consistent color scaling\n",
    "global_vmin = min(np.min(dv) for dv in all_delta_vals)\n",
    "global_vmax = max(np.max(dv) for dv in all_delta_vals)\n",
    "\n",
    "# Re-run plotting, setting norm with global vmin and vmax\n",
    "for row, (model_name, pytorch_model, engine, context, inputs, bindings) in enumerate(models):\n",
    "    for col, W in enumerate(W_values):\n",
    "        ax = axs[row, col]\n",
    "        delta_vals = all_delta_vals[row * len(W_values) + col]\n",
    "        im = ax.imshow(delta_vals, origin=\"lower\", norm=LogNorm(vmin=global_vmin, vmax=global_vmax))\n",
    "\n",
    "# Add row labels with adjusted positions\n",
    "fig.text(0.03, 0.81, 'PyTorch Model', rotation=90, va='center', fontsize=fs-4)\n",
    "fig.text(0.03, 0.52, 'TensorRT FP32', rotation=90, va='center', fontsize=fs-4)\n",
    "fig.text(0.03, 0.22, 'TensorRT FP16', rotation=90, va='center', fontsize=fs-4)\n",
    "\n",
    "# Add colorbar with adjusted position\n",
    "cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "cbar = fig.colorbar(im, cax=cbar_ax, label='Relative Error')\n",
    "cbar.ax.tick_params(labelsize=fs-4)\n",
    "cbar.ax.set_ylabel('Relative Error', fontsize=fs-2)\n",
    "\n",
    "# Clean up TensorRT resources\n",
    "for input_buffer in inputs_fp32:\n",
    "    input_buffer['device'].free()\n",
    "for output_buffer in outputs_fp32:\n",
    "    output_buffer['device'].free()\n",
    "    \n",
    "for input_buffer in inputs_fp16:\n",
    "    input_buffer['device'].free()\n",
    "for output_buffer in outputs_fp16:\n",
    "    output_buffer['device'].free()\n",
    "\n",
    "plt.savefig('../images/plot.pdf')\n",
    "plt.savefig('../images/plot.png', dpi=1200)\n",
    "plt.savefig('../images/plot.svg', dpi=1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
